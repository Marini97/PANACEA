PRISM-games
===========

Version: 3.2.1 (based on PRISM 4.8.dev)
Date: Fri Jul 12 14:27:11 CEST 2024
Hostname: pitagora
Memory limits: cudd=10g, java(heap)=50g
Command line: prism-games -javamaxmem 50g -cuddmaxmem 10g experiments/experiment1/prism/Solo_Defender.prism experiments/experiment1/prism/defender.props -prop 1 -const 'accesstoreverseshell=0:2,webreconsuccesful=0:2,accesstoexecutearbitrarycode=0:2,accesstosensitivefiles=0:2,accesstomysql=0:2' -exportresults 'experiments/experiment1/results/Solo_Defender/Solo_Defender.csv:csv' -exportstrat experiments/experiment1/results/Solo_Defender/Solo_Defender.dot

Parsing model file "experiments/experiment1/prism/Solo_Defender.prism"...

Type:        SMG
Modules:     attacker defender
Variables:   sched DataExfiltration AccesstoReverseShell WebReconSuccesful AccesstoExecuteArbitraryCode AccesstoSensitiveFiles AccesstoMySQL VulnerableApacheHTTPServerVersion SOCKS5ProxyActive MisconfiguredApache UnencryptedFiles WebserverPubliclyExposed CGIscriptsenabled

Parsing properties file "experiments/experiment1/prism/defender.props"...

1 property:
(1) <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.093 secs.
Sorting reachable states list...

Time for model construction: 0.126 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.003 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.018 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.039 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.039 secs.
Sorting reachable states list...

Time for model construction: 0.04 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.008 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.004 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.012 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 200.0

Time for model checking: 0.019 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.036 secs.
Sorting reachable states list...

Time for model construction: 0.036 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 60.0

Time for model checking: 0.006 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.002 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.002 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.003 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 260.0

Time for model checking: 0.01 seconds.

Result: 260.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.015 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.015 secs.
Sorting reachable states list...

Time for model construction: 0.016 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 200.0

Time for model checking: 0.002 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.022 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.002 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.016 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.015 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.016 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.002 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.002 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 200.0

Time for model checking: 0.005 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.001 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 60.0

Time for model checking: 0.004 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.002 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 260.0

Time for model checking: 0.007 seconds.

Result: 260.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 200.0

Time for model checking: 0.004 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.022 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.022 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 200.0

Time for model checking: 0.003 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.026 secs.
Sorting reachable states list...

Time for model construction: 0.026 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.003 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 260.0

Time for model checking: 0.005 seconds.

Result: 260.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.001 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 200.0

Time for model checking: 0.002 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.016 secs.
Sorting reachable states list...

Time for model construction: 0.016 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 200.0

Time for model checking: 0.003 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 60.0

Time for model checking: 0.003 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 260.0

Time for model checking: 0.005 seconds.

Result: 260.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.001 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 200.0

Time for model checking: 0.001 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=0,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.016 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.024 secs.
Sorting reachable states list...

Time for model construction: 0.024 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.021 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.02 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.02 secs.
Sorting reachable states list...

Time for model construction: 0.021 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.012 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=1,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 200.0

Time for model checking: 0.003 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 260.0

Time for model checking: 0.005 seconds.

Result: 260.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.022 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 200.0

Time for model checking: 0.001 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.012 secs.
Sorting reachable states list...

Time for model construction: 0.012 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 200.0

Time for model checking: 0.003 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.002 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 260.0

Time for model checking: 0.005 seconds.

Result: 260.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 200.0

Time for model checking: 0.001 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=0,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.019 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.019 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.018 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.018 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.015 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=1,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 200.0

Time for model checking: 0.003 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 260.0

Time for model checking: 0.004 seconds.

Result: 260.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.001 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 200.0

Time for model checking: 0.001 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=0,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.001 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=256, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=256, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.022 secs.
Sorting reachable states list...

Time for model construction: 0.022 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=128, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=128, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=1,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.017 secs.
Sorting reachable states list...

Time for model construction: 0.017 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
target=0, inf=64, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
target=0, inf=64, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: Infinity

Time for model checking: 0.0 seconds.

Result: Infinity (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.015 secs.
Sorting reachable states list...

Time for model construction: 0.015 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 200.0

Time for model checking: 0.003 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=0,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.014 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 260.0

Time for model checking: 0.004 seconds.

Result: 260.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=1,accesstomysql=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 60.0

Time for model checking: 0.001 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 2.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 200.0

Time for model checking: 0.001 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: accesstoreverseshell=2,webreconsuccesful=2,accesstoexecutearbitrarycode=2,accesstosensitivefiles=2,accesstomysql=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.013 secs.
Sorting reachable states list...

Time for model construction: 0.013 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.5 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"...

Exporting results as list (CSV) to file "experiments/experiment1/results/Solo_Defender/Solo_Defender.csv"...

---------------------------------------------------------------------

Note: There were 486 warnings during computation.

	Command being timed: "../prism-games-3.2.1-linux64-x86/bin/prism -javamaxmem 50g -cuddmaxmem 10g experiments/experiment1/prism/Solo_Defender.prism experiments/experiment1/prism/defender.props -prop 1 -const accesstoreverseshell=0:2,webreconsuccesful=0:2,accesstoexecutearbitrarycode=0:2,accesstosensitivefiles=0:2,accesstomysql=0:2 -exportresults experiments/experiment1/results/Solo_Defender/Solo_Defender.csv:csv -exportstrat experiments/experiment1/results/Solo_Defender/Solo_Defender.dot"
	User time (seconds): 17.16
	System time (seconds): 1.36
	Percent of CPU this job got: 286%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:06.46
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1226512
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 567059
	Voluntary context switches: 15407
	Involuntary context switches: 1150
	Swaps: 0
	File system inputs: 0
	File system outputs: 2024
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
