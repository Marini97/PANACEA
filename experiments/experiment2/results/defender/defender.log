PRISM-games
===========

Version: 3.2.1 (based on PRISM 4.8.dev)
Date: Thu Jun 13 14:52:02 CEST 2024
Hostname: pitagora
Memory limits: cudd=5g, java(heap)=50g
Command line: prism-games -javamaxmem 50g -cuddmaxmem 5g experiments/experiment2/prism/defender.prism experiments/experiment2/prism/properties.props -prop 1 -const 'a6=0:2,a4=0:2,a8=0:2,a2=0:2,a7=0:2,a1=0:2' -exportresults 'experiments/experiment2/results/defender/result.csv:csv' -exportstrat experiments/experiment2/results/defender/defender.dot

Parsing model file "experiments/experiment2/prism/defender.prism"...

Type:        SMG
Modules:     attacker defender
Variables:   sched root A_6 A_4 A_8 A_2 A_7 A_1 A_3

Parsing properties file "experiments/experiment2/prism/properties.props"...

1 property:
(1) <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=0,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.057 secs.
Sorting reachable states list...

Time for model construction: 0.091 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.003 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.018 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=256, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 0.0

Time for model checking: 0.045 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=0,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.011 secs.
Sorting reachable states list...

Time for model construction: 0.012 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.009 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.004 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.014 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.002 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 100.0

Time for model checking: 0.021 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=0,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.006 secs.
Sorting reachable states list...

Time for model construction: 0.006 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=0,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.03 secs.
Sorting reachable states list...

Time for model construction: 0.032 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.002 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.002 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.002 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 40.0

Time for model checking: 0.012 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=0,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.014 secs.
Sorting reachable states list...

Time for model construction: 0.016 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.004 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.006 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.004 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.009 seconds.

Value in the initial state: 140.0

Time for model checking: 0.017 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=0,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.007 secs.
Sorting reachable states list...

Time for model construction: 0.009 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 40.0

Time for model checking: 0.004 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=0,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.007 secs.
Sorting reachable states list...

Time for model construction: 0.007 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=0,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.003 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=0,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=1,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.005 secs.
Sorting reachable states list...

Time for model construction: 0.006 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 500.0

Time for model checking: 0.005 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=1,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.005 secs.
Sorting reachable states list...

Time for model construction: 0.006 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 600.0

Time for model checking: 0.008 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=1,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.004 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.002 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=1,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.005 secs.
Sorting reachable states list...

Time for model construction: 0.006 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 540.0

Time for model checking: 0.006 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=1,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.005 secs.
Sorting reachable states list...

Time for model construction: 0.006 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 640.0

Time for model checking: 0.009 seconds.

Result: 640.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=1,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 540.0

Time for model checking: 0.003 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=1,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.024 secs.
Sorting reachable states list...

Time for model construction: 0.024 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 500.0

Time for model checking: 0.003 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=1,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 600.0

Time for model checking: 0.003 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=1,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=2,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=2,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.002 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=2,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=2,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 40.0

Time for model checking: 0.002 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=2,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 140.0

Time for model checking: 0.003 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=2,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=2,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=2,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=0,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=0,a2=2,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=0,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 20.0

Time for model checking: 0.003 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=0,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.002 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 120.0

Time for model checking: 0.006 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=0,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.002 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=0,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 6 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 60.0

Time for model checking: 0.005 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=0,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 10 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 160.0

Time for model checking: 0.008 seconds.

Result: 160.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=0,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 6 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 60.0

Time for model checking: 0.003 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=0,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 20.0

Time for model checking: 0.002 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=0,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 8 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 120.0

Time for model checking: 0.002 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=0,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=1,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.005 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.002 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.002 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 520.0

Time for model checking: 0.007 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=1,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.003 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.002 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 620.0

Time for model checking: 0.01 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=1,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 520.0

Time for model checking: 0.004 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=1,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.004 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.003 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 560.0

Time for model checking: 0.01 seconds.

Result: 560.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=1,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.005 secs.
Sorting reachable states list...

Time for model construction: 0.006 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.003 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.004 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.004 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.007 seconds.

Value in the initial state: 660.0

Time for model checking: 0.011 seconds.

Result: 660.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=1,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 560.0

Time for model checking: 0.004 seconds.

Result: 560.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=1,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 520.0

Time for model checking: 0.004 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=1,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 620.0

Time for model checking: 0.004 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=1,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 520.0

Time for model checking: 0.001 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=2,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=2,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 120.0

Time for model checking: 0.003 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=2,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=2,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 60.0

Time for model checking: 0.003 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=2,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 160.0

Time for model checking: 0.004 seconds.

Result: 160.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=2,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 60.0

Time for model checking: 0.001 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=2,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=2,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 120.0

Time for model checking: 0.002 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=1,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=1,a2=2,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.002 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 8 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 140.0

Time for model checking: 0.002 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 6 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.002 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 600.0

Time for model checking: 0.002 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 540.0

Time for model checking: 0.003 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 640.0

Time for model checking: 0.003 seconds.

Result: 640.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 540.0

Time for model checking: 0.001 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 600.0

Time for model checking: 0.001 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 140.0

Time for model checking: 0.002 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=0,a8=2,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=0,a8=2,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=0,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 50.0

Time for model checking: 0.003 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=0,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.002 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 150.0

Time for model checking: 0.006 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=0,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=0,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.004 secs.
Sorting reachable states list...

Time for model construction: 0.005 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 90.0

Time for model checking: 0.005 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=0,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 190.0

Time for model checking: 0.009 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=0,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.002 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=0,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.002 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=0,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 150.0

Time for model checking: 0.003 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=0,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=1,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.004 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.002 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 550.0

Time for model checking: 0.006 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=1,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.004 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.002 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 650.0

Time for model checking: 0.008 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=1,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 550.0

Time for model checking: 0.003 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=1,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.004 secs.
Sorting reachable states list...

Time for model construction: 0.005 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.002 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 590.0

Time for model checking: 0.008 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=1,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.003 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 690.0

Time for model checking: 0.009 seconds.

Result: 690.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=1,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 590.0

Time for model checking: 0.004 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=1,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 550.0

Time for model checking: 0.003 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=1,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 650.0

Time for model checking: 0.003 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=1,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.002 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=2,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=2,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.002 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=2,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=2,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 90.0

Time for model checking: 0.003 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=2,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 190.0

Time for model checking: 0.003 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=2,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 90.0

Time for model checking: 0.001 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=2,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=2,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=0,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=0,a2=2,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=0,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 70.0

Time for model checking: 0.005 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=0,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 170.0

Time for model checking: 0.007 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=0,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.002 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=0,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 110.0

Time for model checking: 0.007 seconds.

Result: 110.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=0,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.003 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.003 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 210.0

Time for model checking: 0.009 seconds.

Result: 210.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=0,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 110.0

Time for model checking: 0.004 seconds.

Result: 110.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=0,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 70.0

Time for model checking: 0.003 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=0,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 170.0

Time for model checking: 0.004 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=0,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=1,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.003 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 570.0

Time for model checking: 0.008 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=1,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.003 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.002 seconds.
Expected reachability took 0.007 seconds.

Value in the initial state: 670.0

Time for model checking: 0.01 seconds.

Result: 670.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=1,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 570.0

Time for model checking: 0.003 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=1,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.003 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 610.0

Time for model checking: 0.01 seconds.

Result: 610.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=1,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.004 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.003 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.004 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.003 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.003 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Expected reachability took 0.007 seconds.

Value in the initial state: 710.0

Time for model checking: 0.012 seconds.

Result: 710.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=1,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 610.0

Time for model checking: 0.004 seconds.

Result: 610.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=1,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 570.0

Time for model checking: 0.003 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=1,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 670.0

Time for model checking: 0.004 seconds.

Result: 670.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=1,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 570.0

Time for model checking: 0.002 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=2,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.002 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=2,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 170.0

Time for model checking: 0.004 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=2,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.002 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=2,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 110.0

Time for model checking: 0.003 seconds.

Result: 110.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=2,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.002 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 210.0

Time for model checking: 0.004 seconds.

Result: 210.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=2,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 110.0

Time for model checking: 0.002 seconds.

Result: 110.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=2,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.002 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=2,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 170.0

Time for model checking: 0.002 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=1,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=1,a2=2,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.002 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.002 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.002 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 190.0

Time for model checking: 0.004 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.002 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.002 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 650.0

Time for model checking: 0.003 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.001 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 590.0

Time for model checking: 0.004 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 690.0

Time for model checking: 0.005 seconds.

Result: 690.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 590.0

Time for model checking: 0.002 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.001 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 650.0

Time for model checking: 0.001 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.001 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 90.0

Time for model checking: 0.001 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 190.0

Time for model checking: 0.002 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.001 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=1,a8=2,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=1,a8=2,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 40.0

Time for model checking: 0.002 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 140.0

Time for model checking: 0.003 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 600.0

Time for model checking: 0.003 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 540.0

Time for model checking: 0.002 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 640.0

Time for model checking: 0.004 seconds.

Result: 640.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 540.0

Time for model checking: 0.001 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 600.0

Time for model checking: 0.001 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 140.0

Time for model checking: 0.001 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=0,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=0,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 120.0

Time for model checking: 0.002 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 6 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.003 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 160.0

Time for model checking: 0.004 seconds.

Result: 160.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 6 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 120.0

Time for model checking: 0.001 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 520.0

Time for model checking: 0.002 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 620.0

Time for model checking: 0.003 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 520.0

Time for model checking: 0.002 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 560.0

Time for model checking: 0.003 seconds.

Result: 560.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 660.0

Time for model checking: 0.004 seconds.

Result: 660.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 560.0

Time for model checking: 0.002 seconds.

Result: 560.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 520.0

Time for model checking: 0.002 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 620.0

Time for model checking: 0.002 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 520.0

Time for model checking: 0.001 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 120.0

Time for model checking: 0.001 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 160.0

Time for model checking: 0.002 seconds.

Result: 160.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.001 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 120.0

Time for model checking: 0.0 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=1,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=1,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=0,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=0,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=0,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=0,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=0,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 140.0

Time for model checking: 0.001 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=0,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=0,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=0,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=0,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.001 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=1,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=1,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 600.0

Time for model checking: 0.002 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=1,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=1,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 540.0

Time for model checking: 0.001 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=1,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 640.0

Time for model checking: 0.001 seconds.

Result: 640.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=1,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 540.0

Time for model checking: 0.001 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=1,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=1,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 600.0

Time for model checking: 0.001 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=1,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=2,a7=0,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=2,a7=0,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=2,a7=0,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=2,a7=1,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=2,a7=1,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 140.0

Time for model checking: 0.0 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=2,a7=1,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=2,a7=2,a1=0

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=2,a7=2,a1=1

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=0,a4=2,a8=2,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=0,a4=2,a8=2,a2=2,a7=2,a1=2

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=0,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=128, inf=0, rest=128
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 30.0

Time for model checking: 0.003 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=0,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 130.0

Time for model checking: 0.006 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=0,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=0,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.004 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.002 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 70.0

Time for model checking: 0.005 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=0,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 170.0

Time for model checking: 0.007 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=0,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.002 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=0,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=0,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 130.0

Time for model checking: 0.003 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=0,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=1,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 530.0

Time for model checking: 0.006 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=1,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.003 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 630.0

Time for model checking: 0.008 seconds.

Result: 630.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=1,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 530.0

Time for model checking: 0.002 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=1,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 570.0

Time for model checking: 0.007 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=1,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.009 secs.
Sorting reachable states list...

Time for model construction: 0.01 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 670.0

Time for model checking: 0.008 seconds.

Result: 670.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=1,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 570.0

Time for model checking: 0.003 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=1,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 530.0

Time for model checking: 0.002 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=1,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 630.0

Time for model checking: 0.003 seconds.

Result: 630.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=1,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 530.0

Time for model checking: 0.001 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=2,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=2,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 130.0

Time for model checking: 0.002 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=2,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=2,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.002 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=2,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 170.0

Time for model checking: 0.002 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=2,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=2,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=2,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 130.0

Time for model checking: 0.001 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=0,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=0,a2=2,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=0,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 50.0

Time for model checking: 0.003 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=0,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 150.0

Time for model checking: 0.005 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=0,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.002 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=0,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 90.0

Time for model checking: 0.005 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=0,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 190.0

Time for model checking: 0.007 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=0,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 90.0

Time for model checking: 0.003 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=0,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=0,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 150.0

Time for model checking: 0.003 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=0,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=1,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 550.0

Time for model checking: 0.008 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=1,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 650.0

Time for model checking: 0.007 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=1,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.002 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=1,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 590.0

Time for model checking: 0.008 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=1,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.002 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.003 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Expected reachability took 0.007 seconds.

Value in the initial state: 690.0

Time for model checking: 0.01 seconds.

Result: 690.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=1,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 590.0

Time for model checking: 0.004 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=1,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 550.0

Time for model checking: 0.003 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=1,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 650.0

Time for model checking: 0.004 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=1,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.001 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=2,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.002 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=2,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.002 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=2,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=2,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.002 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=2,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 190.0

Time for model checking: 0.003 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=2,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.001 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=2,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=2,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=1,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=1,a2=2,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 130.0

Time for model checking: 0.001 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.002 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 170.0

Time for model checking: 0.002 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 130.0

Time for model checking: 0.001 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 530.0

Time for model checking: 0.002 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 630.0

Time for model checking: 0.003 seconds.

Result: 630.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 530.0

Time for model checking: 0.001 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 570.0

Time for model checking: 0.003 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 670.0

Time for model checking: 0.003 seconds.

Result: 670.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 570.0

Time for model checking: 0.002 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 530.0

Time for model checking: 0.001 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 630.0

Time for model checking: 0.002 seconds.

Result: 630.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 530.0

Time for model checking: 0.0 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 130.0

Time for model checking: 0.0 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 170.0

Time for model checking: 0.001 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 130.0

Time for model checking: 0.001 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=0,a8=2,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=0,a8=2,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=0,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=64, inf=0, rest=192
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 80.0

Time for model checking: 0.004 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=0,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 180.0

Time for model checking: 0.006 seconds.

Result: 180.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=0,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 80.0

Time for model checking: 0.002 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=0,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 120.0

Time for model checking: 0.006 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=0,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 220.0

Time for model checking: 0.007 seconds.

Result: 220.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=0,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 120.0

Time for model checking: 0.002 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=0,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 80.0

Time for model checking: 0.001 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=0,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 180.0

Time for model checking: 0.002 seconds.

Result: 180.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=0,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 80.0

Time for model checking: 0.0 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=1,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 580.0

Time for model checking: 0.006 seconds.

Result: 580.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=1,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 680.0

Time for model checking: 0.007 seconds.

Result: 680.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=1,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 580.0

Time for model checking: 0.002 seconds.

Result: 580.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=1,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 620.0

Time for model checking: 0.008 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=1,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.002 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.002 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 720.0

Time for model checking: 0.009 seconds.

Result: 720.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=1,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 620.0

Time for model checking: 0.004 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=1,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 580.0

Time for model checking: 0.003 seconds.

Result: 580.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=1,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 680.0

Time for model checking: 0.003 seconds.

Result: 680.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=1,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 580.0

Time for model checking: 0.002 seconds.

Result: 580.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=2,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 80.0

Time for model checking: 0.002 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=2,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 180.0

Time for model checking: 0.002 seconds.

Result: 180.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=2,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.005 secs.
Sorting reachable states list...

Time for model construction: 0.005 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 80.0

Time for model checking: 0.001 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=2,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 120.0

Time for model checking: 0.003 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=2,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 220.0

Time for model checking: 0.004 seconds.

Result: 220.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=2,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 120.0

Time for model checking: 0.001 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=2,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 80.0

Time for model checking: 0.001 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=2,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 180.0

Time for model checking: 0.002 seconds.

Result: 180.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=0,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=0,a2=2,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 80.0

Time for model checking: 0.001 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=0,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.002 seconds.
target=32, inf=0, rest=224
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 8 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.

Value in the initial state: 100.0

Time for model checking: 0.006 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=0,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.004 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.003 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.002 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 200.0

Time for model checking: 0.008 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=0,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 8 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 100.0

Time for model checking: 0.003 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=0,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.005 seconds.

Value in the initial state: 140.0

Time for model checking: 0.008 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=0,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.002 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.003 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 240.0

Time for model checking: 0.01 seconds.

Result: 240.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=0,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 140.0

Time for model checking: 0.004 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=0,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 100.0

Time for model checking: 0.003 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=0,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 200.0

Time for model checking: 0.004 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=0,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=1,a7=0,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.002 seconds.
target=16, inf=0, rest=240
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.002 seconds.
Expected reachability took 0.006 seconds.

Value in the initial state: 600.0

Time for model checking: 0.009 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=1,a7=0,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.002 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.004 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.003 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Expected reachability took 0.007 seconds.

Value in the initial state: 700.0

Time for model checking: 0.011 seconds.

Result: 700.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=1,a7=0,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 600.0

Time for model checking: 0.004 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=1,a7=1,a1=0

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.003 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.003 seconds.
target=8, inf=0, rest=248
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.002 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.002 seconds.
Expected reachability took 0.007 seconds.

Value in the initial state: 640.0

Time for model checking: 0.01 seconds.

Result: 640.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=1,a7=1,a1=1

Computing reachable states... 256 states
Reachable states exploration and model construction done in 0.003 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      256 (1 initial)
Transitions: 577
Choices:     577
Max/avg:     7/2.25
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 13 iterations and 0.003 seconds.
target=4, inf=0, rest=252
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.004 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 13 iterations and 0.003 seconds.
target=4, inf=0, rest=252
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 15 iterations and 0.003 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Expected reachability took 0.007 seconds.

Value in the initial state: 740.0

Time for model checking: 0.012 seconds.

Result: 740.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=1,a7=1,a1=2

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.001 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.001 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 640.0

Time for model checking: 0.004 seconds.

Result: 640.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=1,a7=2,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 600.0

Time for model checking: 0.003 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=1,a7=2,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.001 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.002 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 700.0

Time for model checking: 0.004 seconds.

Result: 700.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=1,a7=2,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 600.0

Time for model checking: 0.002 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=2,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 100.0

Time for model checking: 0.003 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=2,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 200.0

Time for model checking: 0.004 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=2,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=2,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 140.0

Time for model checking: 0.003 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=2,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.002 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.001 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 240.0

Time for model checking: 0.005 seconds.

Result: 240.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=2,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 140.0

Time for model checking: 0.001 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=2,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=2,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 200.0

Time for model checking: 0.002 seconds.

Result: 200.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=1,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=1,a2=2,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 6 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 80.0

Time for model checking: 0.002 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 180.0

Time for model checking: 0.003 seconds.

Result: 180.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 6 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 80.0

Time for model checking: 0.001 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 120.0

Time for model checking: 0.003 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 220.0

Time for model checking: 0.003 seconds.

Result: 220.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 120.0

Time for model checking: 0.001 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 80.0

Time for model checking: 0.0 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 180.0

Time for model checking: 0.002 seconds.

Result: 180.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 80.0

Time for model checking: 0.0 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 580.0

Time for model checking: 0.003 seconds.

Result: 580.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 680.0

Time for model checking: 0.004 seconds.

Result: 680.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 580.0

Time for model checking: 0.001 seconds.

Result: 580.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 620.0

Time for model checking: 0.003 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.001 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.001 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 720.0

Time for model checking: 0.005 seconds.

Result: 720.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 620.0

Time for model checking: 0.001 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 580.0

Time for model checking: 0.001 seconds.

Result: 580.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 680.0

Time for model checking: 0.002 seconds.

Result: 680.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 580.0

Time for model checking: 0.001 seconds.

Result: 580.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 80.0

Time for model checking: 0.001 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 180.0

Time for model checking: 0.002 seconds.

Result: 180.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 80.0

Time for model checking: 0.001 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 120.0

Time for model checking: 0.002 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 220.0

Time for model checking: 0.001 seconds.

Result: 220.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 120.0

Time for model checking: 0.0 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 80.0

Time for model checking: 0.0 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 180.0

Time for model checking: 0.0 seconds.

Result: 180.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=1,a8=2,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=1,a8=2,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 80.0

Time for model checking: 0.0 seconds.

Result: 80.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 130.0

Time for model checking: 0.002 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 70.0

Time for model checking: 0.002 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 170.0

Time for model checking: 0.003 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 130.0

Time for model checking: 0.001 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 530.0

Time for model checking: 0.002 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 630.0

Time for model checking: 0.003 seconds.

Result: 630.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 530.0

Time for model checking: 0.001 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 570.0

Time for model checking: 0.003 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 670.0

Time for model checking: 0.003 seconds.

Result: 670.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 570.0

Time for model checking: 0.002 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 530.0

Time for model checking: 0.001 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 630.0

Time for model checking: 0.002 seconds.

Result: 630.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 530.0

Time for model checking: 0.001 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 130.0

Time for model checking: 0.001 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 170.0

Time for model checking: 0.002 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 130.0

Time for model checking: 0.001 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=0,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=0,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.002 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 150.0

Time for model checking: 0.003 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.003 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.004 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 90.0

Time for model checking: 0.007 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 190.0

Time for model checking: 0.004 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 90.0

Time for model checking: 0.001 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 550.0

Time for model checking: 0.004 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 650.0

Time for model checking: 0.004 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.001 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 590.0

Time for model checking: 0.005 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.002 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.001 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 690.0

Time for model checking: 0.005 seconds.

Result: 690.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 590.0

Time for model checking: 0.002 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.002 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 650.0

Time for model checking: 0.001 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 550.0

Time for model checking: 0.001 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.002 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 190.0

Time for model checking: 0.002 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 90.0

Time for model checking: 0.001 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=1,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=1,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=0,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=0,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 130.0

Time for model checking: 0.001 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=0,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=0,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=0,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 170.0

Time for model checking: 0.001 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=0,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.0 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=0,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=0,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 130.0

Time for model checking: 0.0 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=0,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=1,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 530.0

Time for model checking: 0.001 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=1,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 630.0

Time for model checking: 0.001 seconds.

Result: 630.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=1,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 530.0

Time for model checking: 0.001 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=1,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 570.0

Time for model checking: 0.001 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=1,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 670.0

Time for model checking: 0.002 seconds.

Result: 670.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=1,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 570.0

Time for model checking: 0.001 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=1,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 530.0

Time for model checking: 0.0 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=1,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 630.0

Time for model checking: 0.001 seconds.

Result: 630.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=1,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 530.0

Time for model checking: 0.001 seconds.

Result: 530.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=2,a7=0,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=2,a7=0,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 130.0

Time for model checking: 0.001 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=2,a7=0,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=2,a7=1,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=2,a7=1,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 170.0

Time for model checking: 0.001 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=2,a7=1,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.0 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=2,a7=2,a1=0

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 30.0

Time for model checking: 0.0 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=2,a7=2,a1=1

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 130.0

Time for model checking: 0.0 seconds.

Result: 130.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=1,a4=2,a8=2,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=1,a4=2,a8=2,a2=2,a7=2,a1=2

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 30.0

Time for model checking: 0.001 seconds.

Result: 30.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=128, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.002 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 140.0

Time for model checking: 0.002 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.002 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 600.0

Time for model checking: 0.002 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 540.0

Time for model checking: 0.002 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 640.0

Time for model checking: 0.003 seconds.

Result: 640.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 540.0

Time for model checking: 0.001 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 600.0

Time for model checking: 0.001 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.008 secs.
Sorting reachable states list...

Time for model construction: 0.008 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 140.0

Time for model checking: 0.001 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=0,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=0,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 120.0

Time for model checking: 0.002 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.002 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 160.0

Time for model checking: 0.003 seconds.

Result: 160.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 60.0

Time for model checking: 0.001 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 120.0

Time for model checking: 0.001 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 520.0

Time for model checking: 0.002 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 620.0

Time for model checking: 0.003 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 520.0

Time for model checking: 0.002 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 560.0

Time for model checking: 0.003 seconds.

Result: 560.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 660.0

Time for model checking: 0.004 seconds.

Result: 660.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 560.0

Time for model checking: 0.002 seconds.

Result: 560.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 520.0

Time for model checking: 0.001 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 620.0

Time for model checking: 0.001 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 520.0

Time for model checking: 0.0 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 120.0

Time for model checking: 0.001 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 60.0

Time for model checking: 0.001 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 160.0

Time for model checking: 0.001 seconds.

Result: 160.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 60.0

Time for model checking: 0.001 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 120.0

Time for model checking: 0.001 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=1,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=1,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=0,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=0,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=0,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=0,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=0,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 140.0

Time for model checking: 0.001 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=0,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=0,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=0,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=0,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=1,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=1,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 600.0

Time for model checking: 0.001 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=1,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=1,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 540.0

Time for model checking: 0.001 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=1,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 640.0

Time for model checking: 0.001 seconds.

Result: 640.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=1,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 540.0

Time for model checking: 0.001 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=1,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=1,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 600.0

Time for model checking: 0.0 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=1,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=2,a7=0,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=2,a7=0,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=2,a7=0,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=2,a7=1,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=2,a7=1,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 140.0

Time for model checking: 0.0 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=2,a7=1,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 40.0

Time for model checking: 0.001 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=2,a7=2,a1=0

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=2,a7=2,a1=1

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=0,a8=2,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=0,a8=2,a2=2,a7=2,a1=2

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=64, inf=0, rest=64
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.002 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.002 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 190.0

Time for model checking: 0.003 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 90.0

Time for model checking: 0.0 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 550.0

Time for model checking: 0.002 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 650.0

Time for model checking: 0.003 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 550.0

Time for model checking: 0.002 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 590.0

Time for model checking: 0.004 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 690.0

Time for model checking: 0.004 seconds.

Result: 690.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 590.0

Time for model checking: 0.001 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.001 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 650.0

Time for model checking: 0.002 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 550.0

Time for model checking: 0.0 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.001 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 190.0

Time for model checking: 0.002 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.001 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=0,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=0,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=0,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=32, inf=0, rest=96
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 6 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.002 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=0,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 170.0

Time for model checking: 0.003 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=0,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 6 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=0,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 110.0

Time for model checking: 0.003 seconds.

Result: 110.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=0,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 210.0

Time for model checking: 0.004 seconds.

Result: 210.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=0,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 110.0

Time for model checking: 0.001 seconds.

Result: 110.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=0,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=0,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 170.0

Time for model checking: 0.002 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=0,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.0 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=1,a7=0,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=16, inf=0, rest=112
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 570.0

Time for model checking: 0.003 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=1,a7=0,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.002 seconds.

Value in the initial state: 670.0

Time for model checking: 0.004 seconds.

Result: 670.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=1,a7=0,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 570.0

Time for model checking: 0.001 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=1,a7=1,a1=0

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=8, inf=0, rest=120
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 610.0

Time for model checking: 0.004 seconds.

Result: 610.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=1,a7=1,a1=1

Computing reachable states... 128 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.003 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      128 (1 initial)
Transitions: 257
Choices:     257
Max/avg:     6/2.01
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.001 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 11 iterations and 0.001 seconds.
target=4, inf=0, rest=124
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 13 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Expected reachability took 0.003 seconds.

Value in the initial state: 710.0

Time for model checking: 0.004 seconds.

Result: 710.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=1,a7=1,a1=2

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 610.0

Time for model checking: 0.001 seconds.

Result: 610.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=1,a7=2,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 570.0

Time for model checking: 0.001 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=1,a7=2,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 670.0

Time for model checking: 0.001 seconds.

Result: 670.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=1,a7=2,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 570.0

Time for model checking: 0.0 seconds.

Result: 570.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=2,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=2,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 170.0

Time for model checking: 0.002 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=2,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.0 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=2,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 110.0

Time for model checking: 0.001 seconds.

Result: 110.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=2,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 210.0

Time for model checking: 0.001 seconds.

Result: 210.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=2,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 110.0

Time for model checking: 0.0 seconds.

Result: 110.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=2,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.001 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=2,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 170.0

Time for model checking: 0.001 seconds.

Result: 170.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=1,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=1,a2=2,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 70.0

Time for model checking: 0.0 seconds.

Result: 70.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=0,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=0,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 150.0

Time for model checking: 0.001 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=0,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 4 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=0,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.001 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=0,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 190.0

Time for model checking: 0.001 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=0,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 90.0

Time for model checking: 0.0 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=0,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=0,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 150.0

Time for model checking: 0.0 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=0,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=1,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 550.0

Time for model checking: 0.001 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=1,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 650.0

Time for model checking: 0.001 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=1,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 550.0

Time for model checking: 0.001 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=1,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 590.0

Time for model checking: 0.002 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=1,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.001 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 690.0

Time for model checking: 0.002 seconds.

Result: 690.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=1,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 590.0

Time for model checking: 0.001 seconds.

Result: 590.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=1,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 550.0

Time for model checking: 0.001 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=1,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 650.0

Time for model checking: 0.001 seconds.

Result: 650.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=1,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 550.0

Time for model checking: 0.0 seconds.

Result: 550.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=2,a7=0,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=2,a7=0,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 150.0

Time for model checking: 0.0 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=2,a7=0,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=2,a7=1,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 90.0

Time for model checking: 0.001 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=2,a7=1,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 190.0

Time for model checking: 0.001 seconds.

Result: 190.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=2,a7=1,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 90.0

Time for model checking: 0.0 seconds.

Result: 90.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=2,a7=2,a1=0

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.001 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 50.0

Time for model checking: 0.001 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=2,a7=2,a1=1

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 150.0

Time for model checking: 0.0 seconds.

Result: 150.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=1,a8=2,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=1,a8=2,a2=2,a7=2,a1=2

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 50.0

Time for model checking: 0.0 seconds.

Result: 50.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=0,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=64, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=0,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=0,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=0,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=0,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 140.0

Time for model checking: 0.001 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=0,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=0,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=0,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=0,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=1,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=1,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 600.0

Time for model checking: 0.001 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=1,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=1,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 540.0

Time for model checking: 0.001 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=1,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 640.0

Time for model checking: 0.001 seconds.

Result: 640.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=1,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 540.0

Time for model checking: 0.001 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=1,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=1,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 600.0

Time for model checking: 0.0 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=1,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=2,a7=0,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=2,a7=0,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=2,a7=0,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=2,a7=1,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=2,a7=1,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 140.0

Time for model checking: 0.0 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=2,a7=1,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=2,a7=2,a1=0

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=2,a7=2,a1=1

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=0,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=0,a2=2,a7=2,a1=2

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=0,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=32, inf=0, rest=32
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=0,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 120.0

Time for model checking: 0.001 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=0,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=0,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 60.0

Time for model checking: 0.001 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=0,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 160.0

Time for model checking: 0.001 seconds.

Result: 160.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=0,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 60.0

Time for model checking: 0.0 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=0,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=0,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 120.0

Time for model checking: 0.001 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=0,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=1,a7=0,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=16, inf=0, rest=48
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 520.0

Time for model checking: 0.0 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=1,a7=0,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.001 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 620.0

Time for model checking: 0.002 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=1,a7=0,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 520.0

Time for model checking: 0.001 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=1,a7=1,a1=0

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=8, inf=0, rest=56
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 560.0

Time for model checking: 0.001 seconds.

Result: 560.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=1,a7=1,a1=1

Computing reachable states... 64 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      64 (1 initial)
Transitions: 113
Choices:     113
Max/avg:     5/1.77
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 9 iterations and 0.0 seconds.
target=4, inf=0, rest=60
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 11 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 660.0

Time for model checking: 0.001 seconds.

Result: 660.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=1,a7=1,a1=2

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.001 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 560.0

Time for model checking: 0.001 seconds.

Result: 560.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=1,a7=2,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 520.0

Time for model checking: 0.0 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=1,a7=2,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 620.0

Time for model checking: 0.0 seconds.

Result: 620.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=1,a7=2,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 520.0

Time for model checking: 0.001 seconds.

Result: 520.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=2,a7=0,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=2,a7=0,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 120.0

Time for model checking: 0.001 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=2,a7=0,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.001 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.001 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=2,a7=1,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 60.0

Time for model checking: 0.001 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=2,a7=1,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 160.0

Time for model checking: 0.0 seconds.

Result: 160.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=2,a7=1,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 60.0

Time for model checking: 0.0 seconds.

Result: 60.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=2,a7=2,a1=0

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=2,a7=2,a1=1

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 120.0

Time for model checking: 0.0 seconds.

Result: 120.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=1,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=1,a2=2,a7=2,a1=2

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 20.0

Time for model checking: 0.0 seconds.

Result: 20.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=0,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=0,a7=0,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=32, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=0,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=0,a7=0,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=0,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=0,a7=0,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=0,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=0,a7=1,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=0,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=0,a7=1,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 140.0

Time for model checking: 0.0 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=0,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=0,a7=1,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=0,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=0,a7=2,a1=0

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=0,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=0,a7=2,a1=1

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.002 secs.
Sorting reachable states list...

Time for model construction: 0.002 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=0,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=0,a7=2,a1=2

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.001 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=1,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=1,a7=0,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=16, inf=0, rest=16
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 500.0

Time for model checking: 0.001 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=1,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=1,a7=0,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 600.0

Time for model checking: 0.001 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=1,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=1,a7=0,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=1,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=1,a7=1,a1=0

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.001 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=8, inf=0, rest=24
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 540.0

Time for model checking: 0.001 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=1,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=1,a7=1,a1=1

Computing reachable states... 32 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      32 (1 initial)
Transitions: 49
Choices:     49
Max/avg:     4/1.53
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 7 iterations and 0.0 seconds.
target=4, inf=0, rest=28
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 9 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 640.0

Time for model checking: 0.0 seconds.

Result: 640.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=1,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=1,a7=1,a1=2

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 540.0

Time for model checking: 0.0 seconds.

Result: 540.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=1,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=1,a7=2,a1=0

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=1,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=1,a7=2,a1=1

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 600.0

Time for model checking: 0.0 seconds.

Result: 600.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=1,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=1,a7=2,a1=2

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 5.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 500.0

Time for model checking: 0.0 seconds.

Result: 500.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=2,a7=0,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=2,a7=0,a1=0

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=16, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=2,a7=0,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=2,a7=0,a1=1

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.001 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.001 seconds.

Value in the initial state: 100.0

Time for model checking: 0.001 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=2,a7=0,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=2,a7=0,a1=2

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=2,a7=1,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=2,a7=1,a1=0

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=8, inf=0, rest=8
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=2,a7=1,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=2,a7=1,a1=1

Computing reachable states... 16 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      16 (1 initial)
Transitions: 21
Choices:     21
Max/avg:     3/1.31
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 5 iterations and 0.0 seconds.
target=4, inf=0, rest=12
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 7 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 140.0

Time for model checking: 0.0 seconds.

Result: 140.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=2,a7=1,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=2,a7=1,a1=2

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 40.0

Time for model checking: 0.0 seconds.

Result: 40.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=2,a7=2,a1=0

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=2,a7=2,a1=0

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=8, inf=0, rest=0
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=2,a7=2,a1=1

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=2,a7=2,a1=1

Computing reachable states... 8 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      8 (1 initial)
Transitions: 9
Choices:     9
Max/avg:     2/1.12
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 3 iterations and 0.0 seconds.
target=4, inf=0, rest=4
Computing the upper bound where 1.0 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 5 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 3 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 100.0

Time for model checking: 0.0 seconds.

Result: 100.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

---------------------------------------------------------------------

Model checking: <<attacker,defender>>R{"attacker"}min=? [ F "terminate" ]+R{"defender"}min=? [ F "terminate" ]
Model constants: a6=2,a4=2,a8=2,a2=2,a7=2,a1=2

Warning: Switching to explicit engine to allow strategy generation.

Building model...
Model constants: a6=2,a4=2,a8=2,a2=2,a7=2,a1=2

Computing reachable states... 4 states
Reachable states exploration and model construction done in 0.001 secs.
Sorting reachable states list...

Time for model construction: 0.001 seconds.

Warning: Deadlocks detected and fixed in 1 states

Type:        SMG
States:      4 (1 initial)
Transitions: 4
Choices:     4
Max/avg:     1/1.00
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 1 iterations and 0.0 seconds.
target=4, inf=0, rest=0
Computing the upper bound where 0.0 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (maxmax)...
Prob1 (maxmax) took 1 iterations and 0.0 seconds.
target=4, inf=0, rest=0
Computing the upper bound where 0.7000000000000001 is used instead of 0.0
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (minmin)...
Value iteration (minmin) took 1 iterations and 0.0 seconds.
Expected reachability took 0.0 seconds.

Value in the initial state: 0.0

Time for model checking: 0.0 seconds.

Result: 0.0 (exact floating point)

Exporting strategy as a dot file to file "experiments/experiment2/results/defender/defender.dot"...

Exporting results as list (CSV) to file "experiments/experiment2/results/defender/result.csv"...

---------------------------------------------------------------------

Note: There were 1458 warnings during computation.

	Command being timed: "../prism-games-3.2.1-linux64-x86/bin/prism -javamaxmem 50g -cuddmaxmem 5g experiments/experiment2/prism/defender.prism experiments/experiment2/prism/properties.props -prop 1 -const a6=0:2,a4=0:2,a8=0:2,a2=0:2,a7=0:2,a1=0:2 -exportresults experiments/experiment2/results/defender/result.csv:csv -exportstrat experiments/experiment2/results/defender/defender.dot"
	User time (seconds): 19.38
	System time (seconds): 1.26
	Percent of CPU this job got: 399%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.16
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 884236
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 395819
	Voluntary context switches: 16502
	Involuntary context switches: 2978
	Swaps: 0
	File system inputs: 0
	File system outputs: 5920
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
